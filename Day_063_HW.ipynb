{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Day_063_HW.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meganhsu/ML100-Days/blob/master/Day_063_HW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "PQjXhCGcPQuY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 作業\n",
        "* 在精簡深度學習的方式上 : 卷積類神經 (CNN) 採用像素遠近，而遞歸類神經 (RNN) 採用著則是時間遠近\n",
        "* 那麼，既然有著類似的設計精神，兩者是否有可能互換應用呢?"
      ]
    },
    {
      "metadata": {
        "id": "crB1IBMZPQua",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "#Input array\n",
        "\n",
        "X = np.array([[1, 0, 1, 0], [1 ,0 ,1 ,1 ],[ 0 , 1 , 0 , 1 ]])\n",
        "\n",
        "#Output\n",
        "y = np.array([[1], [1], [0]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DiHphb6MPQug",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Sigmoid Function\n",
        "def sigmoid(x): \n",
        "    return  1/(1+np.exp(-x))\n",
        "\n",
        "#Derivative of Sigmoid Function\n",
        "def derivatives_sigmoid(x): \n",
        "    return x*(1-x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qlXDoBbFPQun",
        "colab_type": "code",
        "colab": {},
        "outputId": "0749be3f-b0f1-4af7-b25a-b1475d6df3aa"
      },
      "cell_type": "code",
      "source": [
        "#Variable initialization\n",
        "epoch = 5000 #Setting training iterations\n",
        "lr = 0.1 #Setting learning rate\n",
        "inputlayer_neurons = X.shape[1] #number of features in data set \n",
        "print(\"inputlayer_neurons=%s\"%(inputlayer_neurons))\n",
        "hiddenlayer_neurons = 3 #number of hidden layers neurons\n",
        "output_neurons = 1 #number of neurons at output layer"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "inputlayer_neurons=4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5IAUxBx3PQuv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#weight and bias initialization\n",
        "random.seed(1)\n",
        "wh = np.random.uniform( size = ( inputlayer_neurons , hiddenlayer_neurons ) ) \n",
        "bh = np.random.uniform( size = ( 1 , hiddenlayer_neurons ) ) \n",
        "wout = np.random.uniform( size = ( hiddenlayer_neurons , output_neurons ) ) \n",
        "bout = np.random.uniform( size = ( 1 , output_neurons ) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RgjSIVgcPQuz",
        "colab_type": "code",
        "colab": {},
        "outputId": "f7aacc2e-9231-4ce8-bbf3-49f3ece2c03d"
      },
      "cell_type": "code",
      "source": [
        "for i in  range ( epoch ) :\n",
        "    #Forward Propogation\n",
        "    hidden_layer_input1 = np.dot(X, wh) \n",
        "    hidden_layer_input = hidden_layer_input1 + bh\n",
        "    hiddenlayer_activations =  sigmoid( hidden_layer_input ) \n",
        "print(X)\n",
        "print(hiddenlayer_activations)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 0 1 0]\n",
            " [1 0 1 1]\n",
            " [0 1 0 1]]\n",
            "[[0.89480338 0.89199728 0.82985343]\n",
            " [0.90035448 0.95145186 0.86513031]\n",
            " [0.78029912 0.8624692  0.79705932]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hcg49p78PQu7",
        "colab_type": "code",
        "colab": {},
        "outputId": "8ce932ee-cfdc-4d22-dcf4-d46cec17ef1c"
      },
      "cell_type": "code",
      "source": [
        "for i in  range ( epoch ) :\n",
        "    '''\n",
        "    寫出output Layer\n",
        "    '''\n",
        "    hidden_layer_input1 = np.dot(hiddenlayer_activations, wout) \n",
        "    hidden_layer_input = hidden_layer_input1 + bout\n",
        "    output = sigmoid( hidden_layer_input ) \n",
        "    \n",
        "print ( \"output of Forward Propogation:\\n{}\" .format(output)) \n",
        "print ( \"wout,bout of Backpropagation:\\n{},\\n{}\" .format(wout, bout ))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "output of Forward Propogation:\n",
            "[[0.83507326]\n",
            " [0.84321734]\n",
            " [0.82405124]]\n",
            "wout,bout of Backpropagation:\n",
            "[[0.36217728]\n",
            " [0.6884675 ]\n",
            " [0.49332898]],\n",
            "[[0.27443878]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lYgWPllsPQvH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}